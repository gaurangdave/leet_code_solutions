{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb78719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 09:34:04.825419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3dccf",
   "metadata": {},
   "source": [
    "# Problem #1: Generating a Normalized Coordinate Grid\n",
    "**Source** : Gemini\n",
    "\n",
    "**Context**\n",
    "In many computer vision tasks, particularly in object detection (e.g., YOLO, SSD), we divide an image into a grid. Each cell in this grid is responsible for predicting objects located within it. To do this, the model needs to know the location of each grid cell. Your task is to generate a tensor that contains the normalized (x, y) coordinates of the center of each cell.\n",
    "\n",
    "**Your Task**\n",
    "Write a Python function generate_normalized_grid(grid_size) that takes an integer grid_size and returns a TensorFlow tensor with the following properties:\n",
    "\n",
    "**Shape**: (grid_size, grid_size, 2)\n",
    "\n",
    "**Data Type**: tf.float32\n",
    "\n",
    "**Content**: The tensor should represent a grid where the last dimension [..., 0] holds the normalized x-coordinates and [..., 1] holds the normalized y-coordinates. The coordinates must be normalized to the range [0.0, 1.0).\n",
    "\n",
    "**Normalization Logic**: For a grid of size N, the center of the cell at (row, col) has coordinates ((col + 0.5) / N, (row + 0.5) / N).\n",
    "\n",
    "**Example**\n",
    "If grid_size = 2, the expected output tensor is:\n",
    "```python\n",
    "<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
    "[[[0.25, 0.25],  # Cell at (row=0, col=0) -> (x=0.25, y=0.25)\n",
    "  [0.75, 0.25]], # Cell at (row=0, col=1) -> (x=0.75, y=0.25)\n",
    "\n",
    " [[0.25, 0.75],  # Cell at (row=1, col=0) -> (x=0.25, y=0.75)\n",
    "  [0.75, 0.75]]]># Cell at (row=1, col=1) -> (x=0.75, y=0.75)\n",
    "```\n",
    "Notice how the x-coordinate increases along the columns and the y-coordinate increases along the rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3a040",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "* So here we need a vectorized solution that creates a tensor whose values depend on its indices.\n",
    "* I think it would help to initialize the grid cells with index values. \n",
    "* We can initialize the grid with zeroes and then use `scatter_nd_update` to update the row cells and column cells.\n",
    "\n",
    "### Update\n",
    "* Found a simpler way to do this, created 2 colums using tf.range and tf.repeat and combined it to create a tensor grid where each cell value represented its coordinate value.\n",
    "* After that calculation was as simple as broadcasting addition and division \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26d845",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a06aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate_normalized_grid(grid_size = 2):\n",
    "    ## step 1 - get the grid indices range\n",
    "    grid_range = tf.range(grid_size, dtype=tf.float32)\n",
    "    ## create column 0 for the grid - this column represents the x-coordinate of each grid cell\n",
    "    ## value indices of this column would be 0,0,1,1 for grid_size 2\n",
    "    col_0 = tf.reshape(tf.repeat(grid_range,repeats=grid_size), shape=(grid_size,grid_size,1))\n",
    "    ## create column 1 for the grid - this column represents y-coordinate of each cell\n",
    "    col_1 = tf.reshape(tf.repeat([grid_range],repeats=grid_size,axis=0),shape=(grid_size,grid_size,1))\n",
    "    ## concatenate to form our grid\n",
    "    ## currently each grid cell represents it index value in float. \n",
    "    grid = tf.concat(values = [col_1,col_0], axis = 2)\n",
    "    ## calculate the grid cell center. \n",
    "    coordinate_grid = (grid + 0.5)/grid_size\n",
    "    return coordinate_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46b7212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760632449.423652   20028 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6053 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:2e:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
       "array([[[0.25, 0.25],\n",
       "        [0.75, 0.25]],\n",
       "\n",
       "       [[0.25, 0.75],\n",
       "        [0.75, 0.75]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_normalized_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1de84a",
   "metadata": {},
   "source": [
    "## Solution 2 - using tf.meshgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d734b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_grid_cooridinates(grid_size = 2):\n",
    "    # 1. Create a 1D vector for x indices: [0, 1, 2, ...]\n",
    "    coorinate_range = tf.range(grid_size, dtype=tf.float32)\n",
    "    grid_X,grid_Y = tf.meshgrid(coorinate_range,coorinate_range)\n",
    "    coordinate_grid = tf.stack(values=[grid_X,grid_Y], axis=2)\n",
    "    return coordinate_grid\n",
    "    \n",
    "def generate_normalized_grid(grid_size = 2):\n",
    "    coordinate_grid = generate_grid_cooridinates(grid_size=grid_size)\n",
    "    normalized_grid = (coordinate_grid + 0.5) / grid_size\n",
    "    return normalized_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc9b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_grid = generate_normalized_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a6684",
   "metadata": {},
   "source": [
    "# Problem #2 : Scaling the Grid to Image Coordinates\n",
    "**Source**: Gemini\n",
    "\n",
    "**Context**\n",
    "In our object detection project, the normalized grid you just created is a generic, resolution-independent representation. However, to actually use it with a specific image, we need to convert those [0.0, 1.0) coordinates into actual pixel coordinates. For example, the center of the top-left cell in a 13x13 grid might be (0.038, 0.038) in normalized space, but on a 416x416 pixel image, that corresponds to pixel (16, 16).\n",
    "\n",
    "**Your Task**\n",
    "Write a Python function `scale_grid_to_pixels(normalized_grid, image_shape)` that takes two arguments:\n",
    "\n",
    "**normalized_grid**: The output tensor from our previous problem, with shape (grid_size, grid_size, 2).\n",
    "\n",
    "**image_shape**: A 1D TensorFlow tensor or a Python tuple/list of two integers, **`[height, width]`**.\n",
    "\n",
    "The function should return a new tensor with the same shape as normalized_grid, but where the (x, y) coordinates have been scaled to the pixel space of the image.\n",
    "\n",
    "**Scaling Logic**:\n",
    "\n",
    "`pixel_x = normalized_x * width`\n",
    "\n",
    "`pixel_y = normalized_y * height`\n",
    "\n",
    "**Example**:\n",
    "Given the normalized_grid for grid_size = 2:\n",
    "```\n",
    "[[[0.25, 0.25], [0.75, 0.25]],\n",
    " [[0.25, 0.75], [0.75, 0.75]]]\n",
    "```\n",
    "And an image_shape of [416, 416], the expected output is:\n",
    "```\n",
    "<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
    "[[[104., 104.],  # (0.25*416, 0.25*416)\n",
    "  [312., 104.]], # (0.75*416, 0.25*416)\n",
    "\n",
    " [[104., 312.],  # (0.25*416, 0.75*416)\n",
    "  [312., 312.]]]># (0.75*416, 0.75*416)\n",
    "```\n",
    "**Important**: Note the order. The image_shape is (height, width), but our coordinate grid is (x, y). Your solution will need to handle this correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f5633",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "* First impression of this problem is that this is straight forward, just a simple multiplication. Lets try that and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69a4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def scale_grid_to_pixels(normalized_grid, image_shape):\n",
    "    ## cast to float32\n",
    "    img_shape_float = tf.cast(image_shape, dtype=tf.float32)\n",
    "    ## reorder the image shape colums so that we multiply normalized_x with width and normalized_y with height\n",
    "    reordered_column = tf.gather(params=img_shape_float, indices=[1,0],axis=0)\n",
    "    scaled_grid = tf.multiply(normalized_grid, reordered_column)\n",
    "    return scaled_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a0d2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
       "array([[[104., 104.],\n",
       "        [312., 104.]],\n",
       "\n",
       "       [[104., 312.],\n",
       "        [312., 312.]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_shape = tf.constant(value=[416,416])\n",
    "scale_grid_to_pixels(normalized_grid=normalized_grid,image_shape=image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e439e0b",
   "metadata": {},
   "source": [
    "# Problem #3: Batch-Scaling Grids for Multiple Images\n",
    "\n",
    "**Context**\n",
    "To train a neural network efficiently, we process multiple images at once in a \"batch\". Our grid scaling logic needs to support this. We'll have one common normalized_grid, but a list of different image shapes—one for each image in the batch. Your task is to perform the scaling operation for the entire batch in a single, vectorized call.\n",
    "\n",
    "**Your Task**\n",
    "Write a function batch_scale_grids(normalized_grid, batch_image_shapes) that takes:\n",
    "\n",
    "`normalized_grid: The (grid_size, grid_size, 2) tensor from Problem #1.`\n",
    "\n",
    "`batch_image_shapes: A 2D tensor of shape (batch_size, 2), where each row is an [height, width] pair.`\n",
    "\n",
    "The function should return a single tensor of shape (batch_size, grid_size, grid_size, 2) containing the scaled grid for each image.\n",
    "\n",
    "**Example**\n",
    "Given normalized_grid (for grid_size=2) and batch_image_shapes:\n",
    "\n",
    "```python\n",
    "# A batch of 2 images with different shapes\n",
    "batch_image_shapes = tf.constant([[416, 416],  # Image 1 is 416x416\n",
    "                                  [800, 600]], # Image 2 is 800x600 (WxH) -> No, (HxW)\n",
    "                                 dtype=tf.int32)\n",
    "```\n",
    "The goal is to multiply the single (2, 2, 2) normalized grid with the (2, 2) batch of shapes to produce a (2, 2, 2, 2) - (batch_size, grid_size, grid_size, 2) output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509d2b9",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "* This is interesting, the shape of both these tensors are different, so direct tensor multiplication won't work. \n",
    "* We can try and use `tf.newaxis` to add axis to normalized grid and then do a tf.reshape to get the desired output. \n",
    "* We'll also need to reorder elements of batch image shape like we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edab5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalized_grid(normalized_grid, batch_image_shapes):\n",
    "    batch_image_shapes_float = tf.cast(batch_image_shapes, dtype=tf.float32)\n",
    "    ## reorder the values\n",
    "    batch_image_shapes_float_reordered = tf.reverse(batch_image_shapes_float, axis=[-1])\n",
    "    ## add additional axis\n",
    "    batch_image_shapes_float_reordered = batch_image_shapes_float_reordered[:,tf.newaxis,tf.newaxis,:]\n",
    "    normalized_grid_expanded = normalized_grid[tf.newaxis,:]\n",
    "    ## this will give us batch_image_shape as (batch_size,1,1,2) and normalized grid as (1,grid_size,grid_size,2)\n",
    "    ## multiplying these two will give us (batch_size, grid_size,grid_size,2)\n",
    "    normalized_grid = tf.multiply(normalized_grid_expanded,batch_image_shapes_float_reordered)\n",
    "    return normalized_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0debd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_image_shapes = tf.constant([[416, 416],  # Image 1 is 416x416\n",
    "                                  [800, 600]], # Image 2 is 800x600 (WxH) -> No, (HxW)\n",
    "                                 dtype=tf.int32)\n",
    "\n",
    "normalized_grids = batch_normalized_grid(normalized_grid=normalized_grid, batch_image_shapes=batch_image_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d668b",
   "metadata": {},
   "source": [
    "# Problem #4: Generating Anchor Box Grids\n",
    "\n",
    "## **Context**\n",
    "\n",
    "So far, we have the coordinates for the *center* of each grid cell. In modern object detectors (like YOLO), each grid cell doesn't just predict one object; it's responsible for several \"anchor boxes\" of different pre-defined shapes and sizes (e.g., a tall box, a wide box, a large square box). The model's job isn't to predict a box from scratch, but rather to predict small *adjustments* to the closest matching anchor box.\n",
    "\n",
    "Our task is to generate the full set of anchor boxes for every grid cell across every image in our batch.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `generate_anchor_grids(scaled_grids, anchor_boxes)` that takes:\n",
    "\n",
    "1.  `scaled_grids`: The output from our previous problem—a tensor of pixel coordinates for the grid centers, with shape `(batch_size, grid_size, grid_size, 2)`.\n",
    "2.  `anchor_boxes`: A 2D tensor of shape `(num_anchors, 2)`, where each row is a `[width, height]` pair for a pre-defined anchor.\n",
    "\n",
    "The function should return a tensor of shape `(batch_size, grid_size, grid_size, num_anchors, 4)`. This final tensor represents the specific bounding boxes (in `[x_min, y_min, x_max, y_max]` format) for every anchor at every grid location.\n",
    "\n",
    "**Calculation Logic**:\n",
    "For each grid center `(cx, cy)` from `scaled_grids` and each anchor size `(w, h)` from `anchor_boxes`:\n",
    "* `x_min = cx - w / 2`\n",
    "* `y_min = cy - h / 2`\n",
    "* `x_max = cx + w / 2`\n",
    "* `y_max = cy + h / 2`\n",
    "\n",
    "**Core Challenge**: This is another broadcasting puzzle, but with more dimensions. You'll need to expand both `scaled_grids` and `anchor_boxes` so you can perform the `center +/- size/2` calculation. After calculating the `min` and `max` coordinates, you will need to combine them to form the final `(..., 4)` dimension.\n",
    "\n",
    "This is the most complex problem yet, but it uses the exact same principles you've already mastered. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae967c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate_anchor_grids(scaled_grids, anchor_boxes):\n",
    "    ## step 1: add axis to scaled grids\n",
    "    reshaped_scaled_grids = scaled_grids[:,:,:,tf.newaxis,:]\n",
    "    ## step 2: add axis to anchor_boxes\n",
    "    reshaped_anchor_boxes = anchor_boxes[tf.newaxis,tf.newaxis,tf.newaxis,:,:]\n",
    "    ## step 3: calculate min values\n",
    "    min_values = reshaped_scaled_grids - reshaped_anchor_boxes/2\n",
    "    max_values = reshaped_scaled_grids + reshaped_anchor_boxes/2\n",
    "    ## step 4: calculate anchor grid\n",
    "    anchor_grid = tf.concat([min_values,max_values],axis=-1)\n",
    "    return anchor_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e62ee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 3, 4), dtype=float32, numpy=\n",
       "array([[[[[ 95.,  95., 105., 105.],\n",
       "          [ 90.,  95., 110., 105.],\n",
       "          [ 95.,  90., 105., 110.]],\n",
       "\n",
       "         [[295.,  95., 305., 105.],\n",
       "          [290.,  95., 310., 105.],\n",
       "          [295.,  90., 305., 110.]]],\n",
       "\n",
       "\n",
       "        [[[ 95., 295., 105., 305.],\n",
       "          [ 90., 295., 110., 305.],\n",
       "          [ 95., 290., 105., 310.]],\n",
       "\n",
       "         [[295., 295., 305., 305.],\n",
       "          [290., 295., 310., 305.],\n",
       "          [295., 290., 305., 310.]]]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Shape: (batch_size=1, grid_size=2, grid_size=2, 2)\n",
    "## i.e. one 2x2 grid => 4 grid centers. \n",
    "scaled_grids = tf.constant(\n",
    "    [[[[100., 100.], [300., 100.]],\n",
    "      [[100., 300.], [300., 300.]]]],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Shape: (num_anchors=3, 2)\n",
    "# i.e. 3 anchors defined with their width and height.\n",
    "anchor_boxes = tf.constant(\n",
    "    [[10., 10.],  # Anchor 1: width=10, height=10\n",
    "     [20., 10.],  # Anchor 2: width=20, height=10\n",
    "     [10., 20.]], # Anchor 3: width=10, height=20\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "anchor_grid = generate_anchor_grids(scaled_grids=scaled_grids,anchor_boxes=anchor_boxes)\n",
    "anchor_grid\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0791741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 90.  95. 110. 105.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(anchor_grid[0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41efae",
   "metadata": {},
   "source": [
    "# Problem #5: Calculating Intersection over Union (IoU)\n",
    "\n",
    "## **Context**\n",
    "\n",
    "Intersection over Union (IoU) is a number from 0 to 1 that measures how much two bounding boxes overlap. It's the ratio of the area of their intersection to the area of their union.\n",
    "\n",
    "IoU is critical for two main reasons:\n",
    "\n",
    "1.  **During Training**: We use IoU to match our generated anchor boxes to the ground truth object boxes. An anchor with a high IoU to a ground truth box is considered a \"positive\" example responsible for predicting that object.\n",
    "2.  **During Inference**: We use IoU in a process called Non-Max Suppression (NMS) to eliminate redundant, overlapping predictions for the same object.\n",
    "\n",
    "Your task is to implement a fully vectorized function that calculates the pairwise IoU for two sets of boxes.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `calculate_iou(boxes1, boxes2)` that takes:\n",
    "\n",
    "1.  `boxes1`: A tensor of shape `(N, 4)` representing N bounding boxes.\n",
    "2.  `boxes2`: A tensor of shape `(M, 4)` representing M bounding boxes.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "  * The box format for both is `[x_min, y_min, x_max, y_max]`.\n",
    "\n",
    "The function should return a 2D tensor of shape `(N, M)`, where `output[i, j]` is the IoU score between `boxes1[i]` and `boxes2[j]`.\n",
    "\n",
    "## **Calculation Logic**\n",
    "\n",
    "This is a multi-step calculation that will require broadcasting to compare every box from `boxes1` with every box from `boxes2`.\n",
    "\n",
    "1.  **Expand Dims for Broadcasting**: Reshape `boxes1` to `(N, 1, 4)` and `boxes2` to `(1, M, 4)`.\n",
    "2.  **Find Intersection Coordinates**:\n",
    "      * The top-left corner of the intersection is `(max(box1_x_min, box2_x_min), max(box1_y_min, box2_y_min))`. Use `tf.maximum`.\n",
    "      * The bottom-right corner is `(min(box1_x_max, box2_x_max), min(box1_y_max, box2_y_max))`. Use `tf.minimum`.\n",
    "3.  **Calculate Intersection Area**:\n",
    "      * Calculate the width and height of the intersection.\n",
    "      * **Crucial Edge Case**: If the boxes don't overlap, the width or height can be negative. You must clip them at 0 (`tf.maximum(width, 0)`). The area is then `width * height`.\n",
    "4.  **Calculate Union Area**:\n",
    "      * Calculate the area of all boxes in `boxes1` and `boxes2`. Area is `(x_max - x_min) * (y_max - y_min)`.\n",
    "      * The union area is `area1 + area2 - intersection_area`.\n",
    "5.  **Calculate IoU**:\n",
    "      * `IoU = intersection_area / union_area`.\n",
    "      * **Crucial Edge Case**: To avoid dividing by zero if the union area is 0, add a tiny number (epsilon, e.g., `1e-7`) to the denominator.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "```python\n",
    "# boxes1 has 2 boxes\n",
    "boxes1 = tf.constant([[0, 0, 10, 10],   # Box A\n",
    "                       [15, 15, 25, 25]], # Box B\n",
    "                      dtype=tf.float32)\n",
    "\n",
    "# boxes2 has 3 boxes\n",
    "boxes2 = tf.constant([[5, 5, 15, 15],     # Box C (overlaps A)\n",
    "                       [0, 0, 10, 10],     # Box D (identical to A)\n",
    "                       [30, 30, 40, 40]],  # Box E (no overlap)\n",
    "                      dtype=tf.float32)\n",
    "```\n",
    "\n",
    "## **Expected Output Shape**: `(2, 3)`\n",
    "\n",
    "## **Sanity Check**\n",
    "\n",
    "  * **IoU of Box A and Box D**: They are identical. The intersection is the area of the box (100), and the union is also the area of the box (100). The IoU should be `1.0`.\n",
    "  * **IoU of Box B and Box E**: They have no overlap. The intersection area is 0. The IoU should be `0.0`.\n",
    "\n",
    "This one brings everything together: broadcasting, element-wise math, slicing, and handling edge cases. Good luck\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e0cadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxes1, boxes2):\n",
    "    # step 1 expand dimensions\n",
    "    reshaped_boxes1 = boxes1[:, tf.newaxis, :]\n",
    "    reshaped_boxes2 = boxes2[tf.newaxis, :, :]\n",
    "\n",
    "    # step 2 calculate top left corner of intersection\n",
    "    # top_left_x = tf.maximum(reshaped_boxes1[:, :, 0], reshaped_boxes2[:, :, 0])\n",
    "    # top_left_y = tf.maximum(reshaped_boxes1[:, :, 1], reshaped_boxes2[:, :, 1])\n",
    "    # top_left = tf.stack([top_left_x, top_left_y], axis=-1)\n",
    "    top_left = tf.maximum(reshaped_boxes1[:, :, 0:2],reshaped_boxes2[:, :, 0:2])\n",
    "    \n",
    "    # step 3 calculate bottom right corner of intersection\n",
    "    # bottom_right_x = tf.minimum(\n",
    "    #     reshaped_boxes1[:, :, 2], reshaped_boxes2[:, :, 2])\n",
    "    # bottom_right_y = tf.minimum(\n",
    "    #     reshaped_boxes1[:, :, 3], reshaped_boxes2[:, :, 3])\n",
    "    # bottom_right = tf.stack([bottom_right_x, bottom_right_y], axis=-1)\n",
    "    bottom_right = tf.minimum(reshaped_boxes1[:, :, 2:],reshaped_boxes2[:, :, 2:])\n",
    "\n",
    "    # step 4 calculate intersection area width\n",
    "    intersection_width = tf.maximum(\n",
    "        (bottom_right[:, :, 0] - top_left[:, :, 0]), 0)\n",
    "\n",
    "    # step 5 calculate intersection height\n",
    "    intersection_height = tf.maximum(\n",
    "        (bottom_right[:, :, 1] - top_left[:, :, 1]), 0)\n",
    "\n",
    "    # step 6 calculate intersection area\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "\n",
    "    # step 7 calculate intersection union\n",
    "    boxes1_area = (reshaped_boxes1[:, :, 2] - reshaped_boxes1[:, :, 0]) * \\\n",
    "        (reshaped_boxes1[:, :, 3] - reshaped_boxes1[:, :, 1])\n",
    "        \n",
    "    boxes2_area = (reshaped_boxes2[:, :, 2] - reshaped_boxes2[:, :, 0]) * \\\n",
    "        (reshaped_boxes2[:, :, 3] - reshaped_boxes2[:, :, 1])        \n",
    "    \n",
    "    union_area = boxes1_area + boxes2_area - intersection_area\n",
    "\n",
    "    ## step 8 calculate iou\n",
    "    epsilon = 1e-7 ## adding epsilon to ensure that denominator is always non-zero.\n",
    "    iou = intersection_area / (union_area + epsilon)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57157c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.14285715, 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boxes1 has 2 boxes\n",
    "boxes1 = tf.constant([[0, 0, 10, 10],   # Box A\n",
    "                       [15, 15, 25, 25]], # Box B\n",
    "                      dtype=tf.float32)\n",
    "\n",
    "# boxes2 has 3 boxes\n",
    "boxes2 = tf.constant([[5, 5, 15, 15],     # Box C (overlaps A)\n",
    "                       [0, 0, 10, 10],     # Box D (identical to A)\n",
    "                       [30, 30, 40, 40]],  # Box E (no overlap)\n",
    "                      dtype=tf.float32)\n",
    "\n",
    "ious= calculate_iou(boxes1=boxes1,boxes2=boxes2)\n",
    "ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e6d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 3), dtype=int32, numpy=\n",
       "array([[[3, 3, 3]],\n",
       "\n",
       "       [[5, 5, 5]]], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "\n",
    "print(t.shape)\n",
    "tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]\n",
    "tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],\n",
    "                                   #   [4, 4, 4]]]\n",
    "tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],\n",
    "                                   #  [[5, 5, 5]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718239e",
   "metadata": {},
   "source": [
    "# Problem #6: Decoding Bounding Box Predictions\n",
    "\n",
    "## **Context**\n",
    "\n",
    "We've now created a comprehensive grid of anchor boxes. A model like YOLO or SSD doesn't learn to predict a box's final coordinates directly. Instead, it learns to predict four small adjustment values—`tx`, `ty`, `tw`, and `th`—relative to each anchor box. This makes the training process more stable.\n",
    "\n",
    "Our job is to take the model's raw output (`tx`, `ty`, `tw`, `th`) and apply it to our anchor boxes to get the final, human-readable bounding box coordinates. This process is called **decoding**.\n",
    "\n",
    "The standard formulas to do this are:\n",
    "\n",
    "  * `predicted_center_x = (tx * anchor_width) + anchor_center_x`\n",
    "  * `predicted_center_y = (ty * anchor_height) + anchor_center_y`\n",
    "  * `predicted_width = anchor_width * exp(tw)`\n",
    "  * `predicted_height = anchor_height * exp(th)`\n",
    "\n",
    "The use of `exp()` for the width and height ensures the final dimensions are always positive.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `decode_predictions(anchor_boxes, predictions)` that takes:\n",
    "\n",
    "1.  `anchor_boxes`: A tensor of anchor boxes, with shape `(B, G, G, A, 4)`, in the format `[x_min, y_min, x_max, y_max]`. `B` is batch size, `G` is grid size, `A` is number of anchors.\n",
    "2.  `predictions`: A tensor of the same shape, `(B, G, G, A, 4)`, containing the model's raw output `[tx, ty, tw, th]`.\n",
    "\n",
    "The function should return a tensor of the decoded boxes, also in `[x_min, y_min, x_max, y_max]` format and with the same shape `(B, G, G, A, 4)`.\n",
    "\n",
    "**The core challenge** in this problem is not broadcasting (the input shapes already match), but **coordinate format conversion**. The decoding formulas require the anchor box to be in `[center_x, center_y, width, height]` format, but the input and output need to be in `[x_min, y_min, x_max, y_max]` format. Your implementation will need to handle these conversions.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "Let's use a very simple case with a batch of 1, a 1x1 grid, and 1 anchor.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# A single anchor box with shape (1, 1, 1, 1, 4)\n",
    "# Corresponds to cx=100, cy=100, w=20, h=10\n",
    "anchor_boxes = tf.constant([[[[90., 95., 110., 105.]]]], dtype=tf.float32)\n",
    "\n",
    "# The model's raw prediction for this anchor box\n",
    "predictions = tf.constant([[[[0.1, -0.2, 0.5, -0.5]]]], dtype=tf.float32)\n",
    "```\n",
    "\n",
    "#### **Sanity Check**\n",
    "\n",
    "With the inputs above, your function should produce a final decoded box of approximately `[85.51, 94.97, 118.49, 101.03]`.\n",
    "\n",
    "I'm ready for your solution\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c0e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(anchor_boxes, predictions):\n",
    "    ## step 1: calculate anchor box centers\n",
    "    anchor_box_centers = (anchor_boxes[...,0:2] + anchor_boxes[...,2:]) / 2\n",
    "    tf.print(\"shape of anchorbox centers\", tf.shape(anchor_box_centers))\n",
    "    ## step 2: calculate anchor box width & height\n",
    "    anchor_box_dimensions = anchor_boxes[...,2:] - anchor_boxes[...,0:2]\n",
    "    tf.print(\"shape of anchor_box_dimensions\", tf.shape(anchor_box_dimensions))\n",
    "    \n",
    "    ## step 3: decode predicted_center\n",
    "    predicted_center = (predictions[...,0:2] * anchor_box_dimensions[...,0:2]) + anchor_box_centers[...,0:2] \n",
    "    \n",
    "    ## step 4: decoded predicted_dimensions\n",
    "    predicted_dimensions = anchor_box_dimensions * tf.exp(predictions[...,2:])\n",
    "    \n",
    "    ## step 5: concat the decoded values    \n",
    "    combined_predictions = tf.concat([predicted_center,predicted_dimensions],axis=-1)\n",
    "    \n",
    "    ## step 6: convert the values to coordinates\n",
    "    decoded_min = ((2 * predicted_center[...,0:2]) - predicted_dimensions[...,0:])/2\n",
    "    decoded_max = ((2 * predicted_center[...,0:2]) + predicted_dimensions[...,0:])/2\n",
    "    tf.print(\"Shape of decoded_min:\", tf.shape(decoded_min))\n",
    "    tf.print(\"Shape of decoded_max:\", tf.shape(decoded_max))\n",
    "    \n",
    "    decoded_predictions = tf.concat([decoded_min,decoded_max], axis=-1)\n",
    "    \n",
    "    return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38f676d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1, 4)\n",
      "(1, 1, 1, 1, 4)\n",
      "shape of anchorbox centers [1 1 1 1 2]\n",
      "shape of anchor_box_dimensions [1 1 1 1 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoded_min: [1 1 1 1 2]\n",
      "Shape of decoded_max: [1 1 1 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 1, 4), dtype=float32, numpy=array([[[[[ 85.51279,  94.96735, 118.48721, 101.03265]]]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single anchor box with the CORRECT shape (1, 1, 1, 1, 4)\n",
    "anchor_boxes = tf.constant([[[[[90., 95., 110., 105.]]]]], dtype=tf.float32)\n",
    "\n",
    "# The model's raw prediction with the CORRECT shape (1, 1, 1, 1, 4)\n",
    "predictions = tf.constant([[[[[0.1, -0.2, 0.5, -0.5]]]]], dtype=tf.float32)\n",
    "\n",
    "print(anchor_boxes.shape)\n",
    "print(predictions.shape)\n",
    "\n",
    "decoded_predictions = decode_predictions(\n",
    "    anchor_boxes=anchor_boxes,\n",
    "    predictions=predictions\n",
    ")\n",
    "decoded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530b6c3",
   "metadata": {},
   "source": [
    "# Problem #7: Filtering Predictions by Confidence Score\n",
    "\n",
    "## **Context**\n",
    "\n",
    "Our `decode_predictions` function produces thousands of potential bounding boxes. However, the vast majority of them are garbage—they don't contain any object. Along with the box coordinates `(tx, ty, tw, th)`, the model also predicts a **confidence score** (often called \"objectness\") for each box. This score, from 0 to 1, represents how sure the model is that an anchor box actually contains an object.\n",
    "\n",
    "The very first step in cleaning up the model's output is to throw away all the boxes with a low confidence score. This is a simple but powerful filter that drastically reduces the number of boxes we need to analyze in later steps.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `filter_by_confidence(decoded_boxes, confidence_scores, threshold)` that takes:\n",
    "\n",
    "1.  `decoded_boxes`: The tensor of decoded boxes from our previous problem, with shape `(B, G, G, A, 4)`.\n",
    "2.  `confidence_scores`: A tensor of confidence scores for each box, with shape `(B, G, G, A, 1)`.\n",
    "3.  `threshold`: A scalar float (e.g., `0.5`). Any box with a score below this threshold should be discarded.\n",
    "\n",
    "The function should return two tensors:\n",
    "\n",
    "1.  `filtered_boxes`: A 2D tensor of shape `(num_good_boxes, 4)` containing only the box coordinates that met the threshold.\n",
    "2.  `filtered_scores`: A 2D tensor of shape `(num_good_boxes, 1)` containing the corresponding scores.\n",
    "\n",
    "**Note:** `num_good_boxes` is the total number of boxes across the entire batch whose confidence score was `>= threshold`. The output tensors are \"flattened,\" containing all good boxes from all images in the batch.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "Let's use a batch of 1, with a 1x2 grid and 2 anchors per cell (4 boxes total).\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Shape: (1, 1, 2, 2, 4)\n",
    "decoded_boxes = tf.constant(\n",
    "    [[[[[10, 10, 20, 20],   # Box 1\n",
    "        [12, 12, 22, 22]],  # Box 2\n",
    "       [[30, 30, 40, 40],   # Box 3\n",
    "        [32, 32, 42, 42]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# Shape: (1, 1, 2, 2, 1)\n",
    "confidence_scores = tf.constant(\n",
    "    [[[[[0.9],  # Score 1\n",
    "        [0.2]], # Score 2\n",
    "       [[0.7],  # Score 3\n",
    "        [0.1]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# A confidence threshold of 0.5\n",
    "threshold = 0.5\n",
    "```\n",
    "\n",
    "## **Sanity Check**\n",
    "\n",
    "With the data above, your function should keep Box 1 (score 0.9) and Box 3 (score 0.7). The expected outputs would be:\n",
    "\n",
    "  * `filtered_boxes` should have a shape of `(2, 4)`.\n",
    "  * `filtered_scores` should have a shape of `(2, 1)`.\n",
    "\n",
    "The core of this problem is figuring out how to use the result of a comparison on one tensor to select elements from another tensor. Ready when you are\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761f816",
   "metadata": {},
   "source": [
    "### Solution 1 - Apply Mask Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "565c0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_confidence(decoded_boxes,confidence_scores,threshold):\n",
    "    ## step 1: calculate the mask\n",
    "    confidence_score_mask = confidence_scores >= threshold\n",
    "        \n",
    "    ## step 2: expand the mask to match decoded boxes shape\n",
    "    confidence_score_mask_repeated = tf.repeat(confidence_score_mask,repeats=4, axis=-1)\n",
    "\n",
    "    ## step 3: create indices for gather_nd\n",
    "    confidence_score_mask_indices = tf.where(confidence_score_mask_repeated)\n",
    "    \n",
    "    ## step 4: apply mask indices to read high score elements\n",
    "    filtered_boxes = tf.gather_nd(decoded_boxes,indices=confidence_score_mask_indices,batch_dims=0)\n",
    "    \n",
    "    ## step 5: reshape to expected output shape.\n",
    "    filtered_boxes = tf.reshape(filtered_boxes, shape=(-1,4))\n",
    "    \n",
    "    ## step 6: extract filtered scores the same way\n",
    "    confidence_score_mask_indices = tf.where(confidence_score_mask)\n",
    "    filtered_scores = tf.gather_nd(confidence_scores,indices=confidence_score_mask_indices,batch_dims=0)\n",
    "    filtered_scores = tf.reshape(filtered_scores, shape=(-1,1))\n",
    "    \n",
    "    return filtered_boxes,filtered_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5940a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[10., 10., 20., 20.],\n",
       "        [30., 30., 40., 40.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[0.9],\n",
       "        [0.7]], dtype=float32)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: (1, 1, 2, 2, 4)\n",
    "decoded_boxes = tf.constant(\n",
    "    [[[[[10, 10, 20, 20],   # Box 1\n",
    "        [12, 12, 22, 22]],  # Box 2\n",
    "       [[30, 30, 40, 40],   # Box 3\n",
    "        [32, 32, 42, 42]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# Shape: (1, 1, 2, 2, 1)\n",
    "confidence_scores = tf.constant(\n",
    "    [[[[[0.9],  # Score 1\n",
    "        [0.2]], # Score 2\n",
    "       [[0.7],  # Score 3\n",
    "        [0.1]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# A confidence threshold of 0.5\n",
    "threshold = 0.5\n",
    "\n",
    "filtered_boxes,filtered_scores = filter_by_confidence(decoded_boxes=decoded_boxes,confidence_scores=confidence_scores,threshold=threshold)\n",
    "filtered_boxes,filtered_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49bf37",
   "metadata": {},
   "source": [
    "### Solution 2 - Using Boolean Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f1f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_confidence(decoded_boxes,confidence_scores,threshold):    \n",
    "    ## step 1: calculate the mask\n",
    "    confidence_score_mask = confidence_scores >= threshold\n",
    "    \n",
    "     # Step 2: Remove the dimension with size 1\n",
    "    mask_4d = tf.squeeze(confidence_score_mask, axis=-1)\n",
    "    \n",
    "    \n",
    "    # 3. Apply the 4D mask to the 5D tensors.\n",
    "    filtered_boxes = tf.boolean_mask(decoded_boxes, mask_4d)\n",
    "    filtered_scores = tf.boolean_mask(confidence_scores, mask_4d)\n",
    "    \n",
    "    return filtered_boxes,filtered_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b13ffb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[10., 10., 20., 20.],\n",
       "        [30., 30., 40., 40.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[0.9],\n",
       "        [0.7]], dtype=float32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: (1, 1, 2, 2, 4)\n",
    "decoded_boxes = tf.constant(\n",
    "    [[[[[10, 10, 20, 20],   # Box 1\n",
    "        [12, 12, 22, 22]],  # Box 2\n",
    "       [[30, 30, 40, 40],   # Box 3\n",
    "        [32, 32, 42, 42]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# Shape: (1, 1, 2, 2, 1)\n",
    "confidence_scores = tf.constant(\n",
    "    [[[[[0.9],  # Score 1\n",
    "        [0.2]], # Score 2\n",
    "       [[0.7],  # Score 3\n",
    "        [0.1]]]]],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# A confidence threshold of 0.5\n",
    "threshold = 0.5\n",
    "\n",
    "filtered_boxes,filtered_scores = filter_by_confidence(decoded_boxes=decoded_boxes,confidence_scores=confidence_scores,threshold=threshold)\n",
    "filtered_boxes,filtered_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9eec58",
   "metadata": {},
   "source": [
    "# Problem #8: Non-Max Suppression (NMS)\n",
    "\n",
    "## **Context**\n",
    "\n",
    "After filtering for confidence, our model might still output multiple, highly confident, overlapping boxes for the same object. For example, when detecting a single cat, we might get three boxes that all look pretty good.\n",
    "\n",
    "We only want to keep the single *best* box and discard the redundant ones. The standard algorithm for this is called **Non-Max Suppression (NMS)**. The name sounds complex, but the idea is simple: for any group of overlapping boxes, find the one with the maximum confidence score and suppress (delete) the rest.\n",
    "\n",
    "The general logic is:\n",
    "\n",
    "1.  Select the box with the highest confidence score.\n",
    "2.  Compare it to all other boxes and discard any that have a high IoU with it.\n",
    "3.  Repeat with the next-highest-scoring box that hasn't been discarded.\n",
    "4.  Continue until all boxes are either selected or discarded.\n",
    "\n",
    "Implementing this efficiently with loops is tricky. As you'd expect, TensorFlow has a built-in, highly-optimized function for this common task. Your challenge is to find this function in the TensorFlow API and use it correctly.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `perform_nms(boxes, scores, max_output_boxes, iou_threshold)` that takes:\n",
    "\n",
    "1.  `boxes`: A 2D tensor of shape `(num_boxes, 4)` in the format `[y_min, x_min, y_max, x_max]`. **Note the `(y, x)` order, which is a common requirement for this operation in TensorFlow.**\n",
    "2.  `scores`: A 1D tensor of shape `(num_boxes,)` containing the confidence score for each box.\n",
    "3.  `max_output_boxes`: A scalar integer for the maximum number of boxes to select.\n",
    "4.  `iou_threshold`: A scalar float (e.g., 0.5). Boxes with an IoU above this threshold relative to a higher-scoring box will be suppressed.\n",
    "\n",
    "The function should return:\n",
    "\n",
    "1.  `selected_indices`: A 1D tensor of integers representing the indices of the boxes that were kept by the NMS algorithm.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "Here are four boxes. Box A and Box B overlap significantly. Box C is nearby. Box D is far away.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note the [y_min, x_min, y_max, x_max] format\n",
    "boxes = tf.constant([[10, 10, 20, 20],  # Box A\n",
    "                     [11, 11, 21, 21],  # Box B (highly overlaps with A)\n",
    "                     [25, 25, 35, 35],  # Box C (separate)\n",
    "                     [100, 100, 110, 110]],# Box D (far away)\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "# Scores for each box\n",
    "scores = tf.constant([0.8, 0.9, 0.75, 0.6], dtype=tf.float32)\n",
    "\n",
    "# Parameters for NMS\n",
    "max_output_boxes = 10\n",
    "iou_threshold = 0.5\n",
    "```\n",
    "\n",
    "## **Sanity Check**\n",
    "\n",
    "The NMS algorithm should perform the following logic:\n",
    "\n",
    "1.  Select Box B (score 0.9), as it has the highest score.\n",
    "2.  Suppress Box A because its IoU with Box B is very high (well above 0.5).\n",
    "3.  Select Box C (score 0.75), the next highest scorer. Its IoU with Box B is 0.\n",
    "4.  Select Box D (score 0.6). Its IoU with both B and C is 0.\n",
    "\n",
    "The indices of the original boxes are `0, 1, 2, 3`. The final selected indices should correspond to Box B, Box C, and Box D. Therefore, the expected output is a tensor containing the indices `[1, 2, 3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2d5ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_nms(boxes, scores, max_output_boxes, iou_threshold):\n",
    "    nm_indices = tf.image.non_max_suppression(boxes,scores,max_output_boxes,iou_threshold)\n",
    "    return nm_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c76cc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the [y_min, x_min, y_max, x_max] format\n",
    "boxes = tf.constant([[10, 10, 20, 20],  # Box A\n",
    "                     [11, 11, 21, 21],  # Box B (highly overlaps with A)\n",
    "                     [25, 25, 35, 35],  # Box C (separate)\n",
    "                     [100, 100, 110, 110]],# Box D (far away)\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "# Scores for each box\n",
    "scores = tf.constant([0.8, 0.9, 0.75, 0.6], dtype=tf.float32)\n",
    "\n",
    "# Parameters for NMS\n",
    "max_output_boxes = 10\n",
    "iou_threshold = 0.5\n",
    "\n",
    "nms_output = perform_nms(boxes=boxes,scores=scores,max_output_boxes=max_output_boxes,iou_threshold=iou_threshold)\n",
    "nms_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b2e45",
   "metadata": {},
   "source": [
    "# Problem #9: Batch-Drawing with Scatter\n",
    "\n",
    "## **Context**\n",
    "\n",
    "A common task is to modify specific regions of a large tensor based on a set of coordinates. Imagine you have a batch of black canvases (3D tensors) and you want to \"draw\" a colored rectangle on each one at a different location—all in a single, vectorized operation without any Python loops. This is a perfect job for a \"scatter\" operation.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `draw_boxes(canvases, boxes, colors)` that takes:\n",
    "\n",
    "1.  `canvases`: A batch of black canvases (all zeros). Shape: `(batch_size, height, width, channels)`.\n",
    "2.  `boxes`: A 2D tensor of box coordinates. Shape: `(batch_size, 4)` in `[y_min, x_min, y_max, x_max]` format. Each row corresponds to a canvas in the batch.\n",
    "3.  `colors`: A 2D tensor of colors. Shape: `(batch_size, channels)`. Each row is the color for the corresponding box.\n",
    "\n",
    "The function should return a single tensor:\n",
    "\n",
    "1.  `updated_canvases`: The batch of canvases with the colored boxes drawn on them. The shape should be unchanged.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Two black 10x10 RGB canvases\n",
    "canvases = tf.zeros((2, 10, 10, 3), dtype=tf.float32)\n",
    "\n",
    "# A 2x4 box to draw on the first canvas, and a 3x3 box on the second\n",
    "boxes = tf.constant([[1, 1, 3, 5],   # Box 1: y in [1,3), x in [1,5)\n",
    "                     [4, 4, 7, 7]],  # Box 2: y in [4,7), x in [4,7)\n",
    "                    dtype=tf.int32)\n",
    "\n",
    "# Color for each box (Red for box 1, Blue for box 2)\n",
    "colors = tf.constant([[1.0, 0.0, 0.0],  # Red\n",
    "                      [0.0, 0.0, 1.0]], # Blue\n",
    "                     dtype=tf.float32)\n",
    "```\n",
    "\n",
    "### **Sanity Check**\n",
    "\n",
    "After running your function, `updated_canvases[0]` should be a black image with a red rectangle, and `updated_canvases[1]` should be a black image with a blue rectangle at the specified coordinates. The challenge is to do this for the entire batch at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a9f12",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "* In order to implement this I need to do the following,\n",
    "    * I need to convert the boxes into x,y coordinates, that covers the range of the boxes. \n",
    "    * I'll then need to add `batch_indices` to the coordinate so that we create the right box for the right batch index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "904f72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def draw_boxes(canvases, boxes, colors):\n",
    "    # step 1: create indices tensor using the boxes for the scatter update\n",
    "    ## create meshgrid for all possible coordinates\n",
    "    y, x = tf.meshgrid(tf.range(10), tf.range(10), indexing=\"ij\")\n",
    "    # universal grid\n",
    "    universal_grid = tf.stack(values=[y, x], axis=-1)\n",
    "\n",
    "    # read the batch size\n",
    "    batch_size = tf.shape(canvases)[0]\n",
    "\n",
    "    # extend the universal grid with batch size\n",
    "    universal_grid = tf.repeat([universal_grid], repeats=batch_size, axis=0)\n",
    "    print(f\"universal_grid.shape {tf.shape(universal_grid)}\")\n",
    "\n",
    "    # create boolean mask based on boxes\n",
    "    new_boxes = boxes[:, tf.newaxis, tf.newaxis, :]\n",
    "    boolean_mask = ((universal_grid[:, :, :, 0] >= new_boxes[:, :, :, 0]) & (universal_grid[:, :, :, 0] < new_boxes[:, :, :, 2])) & ((\n",
    "        universal_grid[:, :, :, 1] >= new_boxes[:, :, :, 1]) & (universal_grid[:, :, :, 1] < new_boxes[:, :, :, 3]))\n",
    "\n",
    "    ## create indices using the boolean mask\n",
    "    boolean_mask_indices = tf.where(boolean_mask)\n",
    "    \n",
    "    # step 2: create updates tensor using the colors tensor\n",
    "    repeated_colors = tf.gather(params=colors, indices=boolean_mask_indices[:,0])\n",
    "    \n",
    "    # step 3: scatter the update\n",
    "    updated_canvas = tf.tensor_scatter_nd_update(canvases,boolean_mask_indices,repeated_colors)\n",
    "    \n",
    "    return updated_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "409391dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universal_grid.shape [ 2 10 10  2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Two black 10x10 RGB canvases\n",
    "canvases = tf.zeros((2, 10, 10, 3), dtype=tf.float32)\n",
    "\n",
    "# A 2x4 box to draw on the first canvas, and a 3x3 box on the second\n",
    "boxes = tf.constant([[1, 1, 3, 5],   # Box 1: y in [1,3), x in [1,5)\n",
    "                     [4, 4, 7, 7]],  # Box 2: y in [4,7), x in [4,7)\n",
    "                    dtype=tf.int32)\n",
    "\n",
    "# Color for each box (Red for box 1, Blue for box 2)\n",
    "colors = tf.constant([[1.0, 0.0, 0.0],  # Red\n",
    "                      [0.0, 0.0, 1.0]],  # Blue\n",
    "                     dtype=tf.float32)\n",
    "\n",
    "updated_canvases = draw_boxes(canvases=canvases, boxes=boxes, colors=colors)\n",
    "\n",
    "# 1. Select the first canvas from the batch and convert to NumPy\n",
    "canvas_to_display = updated_canvases[1].numpy()\n",
    "\n",
    "# 2. Use imshow to display it\n",
    "plt.imshow(canvas_to_display)\n",
    "plt.show() # This command actually renders the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06fd6d5",
   "metadata": {},
   "source": [
    "## Problem \\#10: Batch Image Patch Extraction with `gather_nd`\n",
    "\n",
    "#### **Context**\n",
    "\n",
    "The last problem was about \"writing\" to a tensor using `scatter`. Now let's master the inverse: \"reading\" specific slices from a tensor using `gather`.\n",
    "\n",
    "A common task in computer vision is to extract small patches around a set of keypoints or \"interest points\" from a batch of images. For example, you might want to extract a 5x5 patch around the left eye and right eye from every face in a batch. Your challenge is to do this in a single, vectorized operation.\n",
    "\n",
    "#### **Your Task**\n",
    "\n",
    "Write a function `extract_patches(images, keypoints, patch_size)` that takes:\n",
    "\n",
    "1.  `images`: A batch of images. Shape: `(batch_size, height, width, channels)`.\n",
    "2.  `keypoints`: The center `(y, x)` coordinate for the patch to extract from each image. Shape: `(batch_size, 2)`.\n",
    "3.  `patch_size`: A scalar odd integer (e.g., 3, 5) representing the height and width of the square patch.\n",
    "\n",
    "The function should return a single tensor:\n",
    "\n",
    "1.  `patches`: A tensor containing all the extracted patches. Shape: `(batch_size, patch_size, patch_size, channels)`.\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "Let's create a simple batch of two 10x10 images. The first image will just be its y-coordinates, and the second will be its x-coordinates, so we can easily see where the patches came from.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "height, width = 10, 10\n",
    "y_coords, x_coords = tf.meshgrid(tf.range(height), tf.range(width), indexing='ij')\n",
    "\n",
    "# Image 0: Each pixel's value is its y-coordinate\n",
    "image0 = tf.cast(tf.stack([y_coords]*3, axis=-1), dtype=tf.float32) \n",
    "# Image 1: Each pixel's value is its x-coordinate\n",
    "image1 = tf.cast(tf.stack([x_coords]*3, axis=-1), dtype=tf.float32)\n",
    "\n",
    "images = tf.stack([image0, image1], axis=0) # Shape (2, 10, 10, 3)\n",
    "\n",
    "# Keypoint for each image in (y, x) format\n",
    "keypoints = tf.constant([[3, 4],  # Center of patch for image 0\n",
    "                         [6, 5]], # Center of patch for image 1\n",
    "                        dtype=tf.int32)\n",
    "\n",
    "patch_size = 3\n",
    "```\n",
    "\n",
    "#### **Sanity Check**\n",
    "\n",
    "The keypoint for the first image is `(y=3, x=4)`. A 3x3 patch centered here will cover y-coordinates `[2, 3, 4]` and x-coordinates `[3, 4, 5]`. Since the pixel values in the first image are just their y-coordinate, the final patch should look like a `3x3` grid where the first row is all `2`s, the second row is all `3`s, and the third row is all `4`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "952cbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_patches(images, keypoints, patch_size):\n",
    "#     images_shape = tf.shape(images)\n",
    "#     batch_size = images_shape[0]\n",
    "#     image_height = images_shape[1]\n",
    "#     image_width = images_shape[2]\n",
    "#     images_channel = images_shape[3]\n",
    "\n",
    "#     # step 1: convert keypoints to min/max coordinates\n",
    "#     float_keypoints = tf.cast(keypoints, dtype=tf.float32)\n",
    "#     # TODO: We might have to add logic to make sure min max doesn't go over the image boundry\n",
    "#     y_min = tf.cast(\n",
    "#         tf.floor(float_keypoints[:, 0] - (patch_size) / 2), dtype=tf.int32)\n",
    "#     y_max = tf.cast(\n",
    "#         tf.floor(float_keypoints[:, 0] + (patch_size) / 2), dtype=tf.int32)\n",
    "#     x_min = tf.cast(\n",
    "#         tf.floor(float_keypoints[:, 1] - (patch_size) / 2), dtype=tf.int32)\n",
    "#     x_max = tf.cast(\n",
    "#         tf.floor(float_keypoints[:, 1] + (patch_size) / 2), dtype=tf.int32)\n",
    "#     min_max_coordinates = tf.stack([y_min, x_min, y_max, x_max], axis=-1)\n",
    "#     print(min_max_coordinates.shape)\n",
    "\n",
    "#     # step 2: create universal grid\n",
    "#     y, x = tf.meshgrid(tf.range(image_height),\n",
    "#                        tf.range(image_width), indexing=\"ij\")\n",
    "#     grid = tf.stack([y, x], axis=-1)\n",
    "#     universal_grid = tf.repeat([grid], repeats=batch_size, axis=0)\n",
    "#     print(universal_grid.shape)\n",
    "\n",
    "#     # step 3: create boolean mask from universal grid for minmax coordinates\n",
    "#     min_max_expanded_dims = min_max_coordinates[:, tf.newaxis, tf.newaxis, :]\n",
    "#     boolean_mask = (universal_grid[:, :, :, 0] >= min_max_expanded_dims[:, :, :, 0]) & (universal_grid[:, :, :, 0] <= min_max_expanded_dims[:, :, :, 2]) & (\n",
    "#         universal_grid[:, :, :, 1] >= min_max_expanded_dims[:, :, :, 1]) & (universal_grid[:, :, :, 0] <= min_max_expanded_dims[:, :, :, 3])\n",
    "#     boolean_mask_indices = tf.where(boolean_mask)\n",
    "\n",
    "#     # step 4: use the mask to extract the data using gather or gather nd.\n",
    "#     extracted_patches = tf.gather_nd(\n",
    "#         params=images, indices=boolean_mask_indices, batch_dims=0)\n",
    "#     # temp = tf.reshape(extracted_patches, shape=(-1,\n",
    "#     #                   patch_size, patch_size, images_channel))\n",
    "#     return extracted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03717abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(images, keypoints, patch_size):\n",
    "    images_shape = tf.shape(images)\n",
    "    batch_size = images_shape[0]\n",
    "    image_height = images_shape[1]\n",
    "    image_width = images_shape[2]\n",
    "    images_channel = images_shape[3]\n",
    "    # step 1: create a patch\n",
    "    patch_grid_range = patch_size // 2\n",
    "    patch_y,patch_x = tf.meshgrid(tf.range(-patch_grid_range,patch_grid_range+1),tf.range(-patch_grid_range,patch_grid_range+1),indexing=\"ij\")\n",
    "    patch_grid = tf.stack([patch_y,patch_x], axis=-1)\n",
    "\n",
    "    # step 2: create image patch\n",
    "    keypoints_extra_dims = keypoints[:,tf.newaxis,tf.newaxis,:]\n",
    "    image_patch_indices = tf.add(keypoints_extra_dims,patch_grid)\n",
    "    \n",
    "    # step 3: extract patches\n",
    "    print(images.shape)\n",
    "    print(image_patch_indices.shape)\n",
    "    batch_indices = tf.range(batch_size, dtype=tf.int32)\n",
    "    batch_indices = batch_indices[:,tf.newaxis,tf.newaxis,tf.newaxis]\n",
    "\n",
    "    ones_tensor = tf.ones(shape=(batch_size,patch_size,patch_size,1), dtype=tf.int32)\n",
    "    stretched_batch_indices = tf.multiply(ones_tensor,batch_indices)\n",
    "\n",
    "    patch_with_batch_indices = tf.concat([stretched_batch_indices,image_patch_indices],axis=-1)\n",
    "    patches = tf.gather_nd(images, patch_with_batch_indices, batch_dims=0)\n",
    "    return patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8a517cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 10, 3)\n",
      "(2, 3, 3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 3, 3), dtype=float32, numpy=\n",
       "array([[[[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3.],\n",
       "         [3., 3., 3.],\n",
       "         [3., 3., 3.]],\n",
       "\n",
       "        [[4., 4., 4.],\n",
       "         [4., 4., 4.],\n",
       "         [4., 4., 4.]]],\n",
       "\n",
       "\n",
       "       [[[4., 4., 4.],\n",
       "         [5., 5., 5.],\n",
       "         [6., 6., 6.]],\n",
       "\n",
       "        [[4., 4., 4.],\n",
       "         [5., 5., 5.],\n",
       "         [6., 6., 6.]],\n",
       "\n",
       "        [[4., 4., 4.],\n",
       "         [5., 5., 5.],\n",
       "         [6., 6., 6.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = 10, 10\n",
    "y_coords, x_coords = tf.meshgrid(tf.range(height), tf.range(width), indexing='ij')\n",
    "\n",
    "# Image 0: Each pixel's value is its y-coordinate\n",
    "image0 = tf.cast(tf.stack([y_coords]*3, axis=-1), dtype=tf.float32) \n",
    "# Image 1: Each pixel's value is its x-coordinate\n",
    "image1 = tf.cast(tf.stack([x_coords]*3, axis=-1), dtype=tf.float32)\n",
    "\n",
    "images = tf.stack([image0, image1], axis=0) # Shape (2, 10, 10, 3)\n",
    "\n",
    "# Keypoint for each image in (y, x) format\n",
    "keypoints = tf.constant([[3, 4],  # Center of patch for image 0\n",
    "                         [6, 5]], # Center of patch for image 1\n",
    "                        dtype=tf.int32)\n",
    "\n",
    "patch_size = 3\n",
    "\n",
    "extracted_patches = extract_patches(images=images,keypoints=keypoints,patch_size=patch_size)\n",
    "extracted_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c4806",
   "metadata": {},
   "source": [
    "# Problem #11: Batched Tensor Rolling\n",
    "\n",
    "## **Context**\n",
    "\n",
    "A \"roll\" or \"circular shift\" is an operation where the pixels of an image that are shifted off one edge wrap around to the opposite edge. It's a common data augmentation technique. The challenge is to perform a *different* roll for each image in a batch.\n",
    "\n",
    "## **Your Task**\n",
    "\n",
    "Write a function `batch_roll(images, shifts)` that takes:\n",
    "\n",
    "1.  `images`: A batch of images. Shape: `(batch_size, height, width, channels)`.\n",
    "2.  `shifts`: The `(dy, dx)` shifts for each image. Shape: `(batch_size, 2)`. `dy` is the vertical shift, `dx` is the horizontal shift.\n",
    "\n",
    "The function should return a single tensor:\n",
    "\n",
    "1.  `rolled_images`: The batch of images, with each image shifted according to its corresponding `(dy, dx)` vector. The shape should be unchanged.\n",
    "\n",
    "### **Core Challenge**\n",
    "\n",
    "This is a pure `gather` problem. For each destination pixel `(y, x)` in the output, you need to calculate which pixel `(source_y, source_x)` to grab from the input image. The circular wrapping can be achieved with the modulo operator (`%`).\n",
    "\n",
    "-----\n",
    "\n",
    "### Test Data\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple 5x5 image with a cross shape\n",
    "image = tf.constant([[0,0,1,0,0],\n",
    "                     [0,0,1,0,0],\n",
    "                     [1,1,1,1,1],\n",
    "                     [0,0,1,0,0],\n",
    "                     [0,0,1,0,0]], dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=-1) # Add channel dim -> (5,5,1)\n",
    "\n",
    "# Create a batch of two identical images\n",
    "images = tf.stack([image, image], axis=0) # Shape (2, 5, 5, 1)\n",
    "\n",
    "# Shift the first image down by 1 and right by 1\n",
    "# Shift the second image up by 1 and left by 2\n",
    "shifts = tf.constant([[1, 1],   # [dy, dx] for image 0\n",
    "                     [-1, -2]], # [dy, dx] for image 1\n",
    "                    dtype=tf.int32)\n",
    "```\n",
    "\n",
    "### **Sanity Check**\n",
    "\n",
    "The cross in the first output image (`rolled_images[0]`) should be shifted down and to the right, with the parts that go off the edge wrapping around to the top and left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9b2a6",
   "metadata": {},
   "source": [
    "#### Thoughts\n",
    "* Here shifts is not the index of row or columns, but rather magnitude and direction of shift\n",
    "* So if the shift is positive we need to go down or right,i.e. we need to roll last `n` rows or columns to the top. If shift is 1 then roll 1 last 1 row if shift is 2 roll last 2 rows\n",
    "* Similarly if the shift is negative we need to go up or left and so need to roll first `n` rows or columns to the bottom. \n",
    "* Negating the shift values will tell us which rows to roll and their values will tell us where to shift. I think.\n",
    "* I was wrong here, `gather_nd` does not work with negative indices. \n",
    "* Also to read the columns, I'll need seperate indices for each row and column value. \n",
    "\n",
    "#### New Approach\n",
    "* We need to use modulo operator to find the threshold index which can be used to move the rows and columns for the roll\n",
    "* Lets assume 5x5 image\n",
    "\n",
    "##### Vertical Roll\n",
    "\n",
    "```bash\n",
    "Shifting Up with -tve shifts \n",
    "-5%5,-4%5,-3%5,-2%5,-1%5 = (0, 1, 2, 3, 4)\n",
    "\n",
    "- -5 means do nothing\n",
    "- -4 means shift 4 rows up, which is same as moving the last row to the top. Which means create a mask to move `row index 4 (last row)` to the top. \n",
    "- -3 means shift 3 rows up, which is same as moving last 2 rows to the top. Which means create a mask to move `row index 3 and 4 (last 2 rows)` to the top. \n",
    "- -2 means shift 2 rows up, which is same as moving last 3 rows to the top. Which means create a mask to move `row index 2, 3 and 4 (last 3 rows)` to the top. \n",
    "- This can be generalized by using the modulo value. Subtracting modulo value from total rows gives us the threshold index above which are the shift indices and below which are the roll indices. \n",
    "- so for -4, module is 1, so threshold is 5 - 1 = 4, move rows 4 and above to the top. \n",
    "- for -3, modulo is 2, so threshold is 5 - 2 = 3, move rows 3 and above to the top\n",
    "\n",
    "Shifting Down with +tve shifts \n",
    "\n",
    "5%5,4%5,3%5,2%5,1%5 = (0, 4, 3, 2, 1)\n",
    "\n",
    "- 5 means do nothing\n",
    "- 4 means shift 4 rows down, which is same as moving last 4 rows to the top i.e. threshold would be 5 - 4 = 1, move rows 1 and above to the top\n",
    "- 3 means shift 3 rows down, which is same as moving last 3 rows to the top i.e. threshold would be 5 - 3 = 2 move rows 2 and above to the top\n",
    "```\n",
    "\n",
    "##### Horizontal roll\n",
    "* We can use similar approach as positive roll to this\n",
    "\n",
    "```bash\n",
    "Shifting left with -tive shift \n",
    "-5%5,-4%5,-3%5,-2%5,-1%5 = (0, 1, 2, 3, 4)\n",
    "\n",
    "- -5 means do nothing\n",
    "- -4 means shift 4 cols  left, is same is moving last col to the left most position. i.e. threshold would be 5 - 1 = 4 move columns 4 and above to the left\n",
    "- -3 means shift 3 cols  left, is same is moving last 2 col to the left most position. i.e. threshold would be 5 - 2 = 3 move columns 3 and above to the left\n",
    "- and so on\n",
    "\n",
    "Shifting right with +tive shift \n",
    "\n",
    "5%5,4%5,3%5,2%5,1%5 = (0, 4, 3, 2, 1)\n",
    "\n",
    "- 5 means do nothing\n",
    "- 4 emans shift 4 cols right, is same as moving last 4 colors to the left most position. i.e threshold would be 5 - 4 = 1, move columns 1 and above to the left. \n",
    "- and so on. \n",
    "\n",
    "```\n",
    "* We can then create boolean mask to select rows and columns\n",
    "* Ok this won't work because `tf.where` and `tf.gather` looses the 2D structure of the image. \n",
    "* We just have to use the formula `source = (destination - dy)%height` to map source pixels to destination pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b4747c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_roll(images, shifts):\n",
    "    ## get input shape\n",
    "    batch_size,image_width,image_height, channels = images.shape.as_list()\n",
    "    \n",
    "    \n",
    "    ## create blank universal grid\n",
    "    y, x = tf.meshgrid(tf.range(image_width), tf.range(image_height), indexing=\"ij\")\n",
    "    # universal grid\n",
    "    universal_grid = tf.stack(values=[y, x], axis=-1)\n",
    "    # extend the universal grid with batch size\n",
    "    universal_grid = tf.repeat([universal_grid], repeats=batch_size, axis=0)\n",
    "    \n",
    "    # extend shifts for broadcasting\n",
    "    extended_shifts = shifts[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "    vertical_shift_sources = (universal_grid[:,:,:,0] - extended_shifts[:,:,:,0]) % image_height\n",
    "    horizontal_shift_sources = (universal_grid[:,:,:,1] - extended_shifts[:,:,:,1]) % image_width\n",
    "    \n",
    "    shift_sources = tf.stack([vertical_shift_sources,horizontal_shift_sources],axis = -1)\n",
    "    print(shift_sources.shape)\n",
    "\n",
    "    batch_indices = tf.range(batch_size, dtype=tf.int32)\n",
    "    batch_indices = batch_indices[:,tf.newaxis,tf.newaxis,tf.newaxis]\n",
    "\n",
    "    ones_tensor = tf.ones(shape=(batch_size,image_height,image_width,1), dtype=tf.int32)\n",
    "    stretched_batch_indices = tf.multiply(ones_tensor,batch_indices)\n",
    "\n",
    "    \n",
    "    shift_sources_batch_indices = tf.concat([stretched_batch_indices,shift_sources],axis=-1)\n",
    "    print(shift_sources_batch_indices.shape)\n",
    "    print(images.shape)\n",
    "    rolled_images = tf.gather_nd(images,indices=shift_sources_batch_indices,batch_dims=0)\n",
    "    return rolled_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "55744b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5, 2)\n",
      "(2, 5, 5, 3)\n",
      "(2, 5, 5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 5, 1), dtype=float32, numpy=\n",
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple 5x5 image with a cross shape\n",
    "image = tf.constant([[0,0,1,0,0],\n",
    "                     [0,0,1,0,0],\n",
    "                     [1,1,1,1,1],\n",
    "                     [0,0,1,0,0],\n",
    "                     [0,0,1,0,0]], dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=-1) # Add channel dim -> (5,5,1)\n",
    "\n",
    "# Create a batch of two identical images\n",
    "images = tf.stack([image, image], axis=0) # Shape (2, 5, 5, 1)\n",
    "\n",
    "# Shift the first image down by 1 and right by 1\n",
    "# Shift the second image up by 1 and left by 2\n",
    "shifts = tf.constant([[1, 1],   # [dy, dx] for image 0\n",
    "                     [-1, -2]], # [dy, dx] for image 1\n",
    "                    dtype=tf.int32)\n",
    "\n",
    "rolled_images = batch_roll(images=images,shifts=shifts)\n",
    "rolled_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22f17658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1  1], shape=(2,), dtype=int32)\n",
      "rows_roll_indices.shape (2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.constant([[0,0,1,0,0],\n",
    "                     [0,0,1,0,0],\n",
    "                     [1,1,1,1,1],\n",
    "                     [0,0,1,0,0],\n",
    "                     [0,0,1,0,0]], dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=-1) # Add channel dim -> (5,5,1)\n",
    "\n",
    "# Create a batch of two identical images\n",
    "images = tf.stack([image, image], axis=0) # Shape (2, 5, 5, 1)\n",
    "\n",
    "# Shift the first image down by 1 and right by 1\n",
    "# Shift the second image up by 1 and left by 2\n",
    "shifts = tf.constant([[1, 1],   # [dy, dx] for image 0\n",
    "                     [-1, -2]], # [dy, dx] for image 1\n",
    "                    dtype=tf.int32)\n",
    "# create indices for rows to roll\n",
    "## read the vertical shifts\n",
    "rows_roll_indices = shifts[:,0]\n",
    "## negate the shift value to get indices range to roll\n",
    "rows_roll_indices = rows_roll_indices * -1\n",
    "print(rows_roll_indices)\n",
    "print(f\"rows_roll_indices.shape {rows_roll_indices.shape}\")\n",
    "\n",
    "## hardcoded POC\n",
    "## shifting down by 1\n",
    "## read last row\n",
    "last_row_index = [0,4] ## 0th image, 5th row\n",
    "# rows_to_shift = [0,0],[0,1],[0,2],[0,3]\n",
    "gathered_stuff = tf.gather_nd(images,indices=[[[0,4],[0,0],[0,1],[0,2],[0,3]]],batch_dims=0)\n",
    "gathered_stuff\n",
    "\n",
    "## reading last column from gathered stuff\n",
    "gathered_stuff_col = tf.gather_nd(gathered_stuff,indices=[[[0,0,0],[0,1,0],[0,2,0],[0,3,0],[0,4,0]]],batch_dims=0)\n",
    "gathered_stuff_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b176c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 5, 1]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9020c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3, 4)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## shifting up\n",
    "-5%5,-4%5,-3%5,-2%5,-1%5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5038957b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4, 3, 2, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5%5,4%5,3%5,2%5,1%5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
